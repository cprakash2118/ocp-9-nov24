#Quick Recap:-
- Product - Red Hat OpenShift 
- ROL 
- Classroom Environment.
  1.Users 
    - student/student; root/redhat - VMs login 
    - admin/redhatocp ; developer/developer - OCP cluster login 	
	- kubeadmin/passowrd-file 
	  $ ssh root@utility 
      # cd /home/lab/ocp4/auth ; scp -r kube* student@workstation:/home/student 
      # cat kubeadmin | grep server 
      # cat kubeadmin-passwd 
    - oc login -u userName -p Password API-URL
     $oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
     Note: UserName, Password and API-URL required to login to OCP cluster using Oauth Service.

   2. Network/Domain 
    - lab.example.com  - 172.25.250.0/24 
	- ocp4.example.com - 192.168.50.0/24 
	- example.com      - 172.25.252.0/24 
	
	As per OCP - baseDomain - example.com ; clusterName - ocp4 
	As per DNS - domain in example.com, lab.example.com & ocp4.example.com
	
   3. VMs details 
      - workstation.lab.example.com  [ jump server - Linux/Window ]
	    FYI- Login to OCP cluster.
	  
	  - utility.lab.example.com [HAproxy, DNS, DHCP, PXE for OCP, Apache-httpd, etc ]
	    FYI- Having 2 NIC - eth0-172.25.250.0/24 ; eth1-192.168.50.0/24; eth1 connect to CLuster Network.
	  - registry.ocp4.example.com 
	  - idm.ocp4.example.com 
	  - master01.ocp4.example.com 
	  
      - bastion.lab.example.com [ it connect Student Network to Classroom Network - then connect to internet]
        FYI- Why internal required - quay.io; github.com; registry.redhat.io 
      - classroom.example.com  [This connect to internet]

- Login to OCP cluster:-
  - Login to OCP cluster via CLI.
  $ oc login -u admin -p redhatocp https://api.ocp4.example.com:6443 
  $ oc login -u kubeadmin -p Password-From-kubeadmin-passwd-file https://api.ocp4.example.com:6443
  
  - Login to OCP cluster using Web Console.
  $ oc whoami 
  admin 
  $ oc whoami --show-console 
  https://console-openshift-console.apps.ocp4.example.com 
  
  You admin/developer/kubeadmin account.
  
  API URL     - https://api.ClusterName.BaseDomain:6443
  Web Console - https://appName-NameSpace.apps.ClusterName.BaseDomain

  Eg:
  API URL for lab:-
   ClusterName - ocp4 
   BaseDomain  - example.com 
  https://api.ocp4.example.com:6443 

  Why 6443 port only ?
  ssh root@utility 
  lsof -i:6443 
  vim /etc/haproxy/haproxy.conf 
  
  Login to HAproxy - https://utility.lab.example.com:32700
               admin/RedH@999

 Web Console for lab:-
    appName     - console 
	NameSpace   - openshift-console
	ClusterName - ocp4 
    BaseDomain  - example.com 
  https://console-openshift-console.apps.ocp4.example.com
  
  
   
	
- OCP Operations 


$ oc whoami 
Login-Account

$ oc whoami -t 
TOKEN

$ oc whoami --show-console
https://console-openshift-console.apps.ocp4.example.com

$ oc version 
$ oc get clusterversion 
$ oc get nodes 
$ oc debug node/master 
#chroot /host 
# hostname -f
master0.ocp4.example.com 
# cat /etc/redhat-r<tab>
RHEL CoreOS 4.14

Note: RHEL CoreOS version is Red Hat OpenShift version.



#Applications deployment:-
1. Build Image & Applications outside (As images) and deploy on OCP cluster via POD. 

- Build your custom image.
vim Containerfile 
 FROM ubi8/ubi:8.3
 MAINTAINER Your Name  <_youremail_>
 LABEL description="A custom Apache container based on UBI 8"
 RUN yum install -y httpd && \
    yum clean all
 EXPOSE 80
 RUN echo "Hello from Containerfile" > /var/www/html/index.html
 CMD ["httpd", "-D", "FOREGROUND"]

- podman images 
- podman build --layer=false -t my-image-name .
- oc new-app --name application01 image-name

2. Build Image & Applications within OCP and deploy on OCP cluster via POD.
    CI/CD example 
	
- oc new-project demo02
- oc project -q
- oc new-app --name test2 httpd:2.4~https://gihub.com/cprakash2118/web 
$ oc get pods; svc; dc, bc 	

###
- login to OCP cluster 
- Login as admin/developer/kubeadmin
  FYI- admin/kubeadmin are "cluster-admin" role; developer member.
- Basic operations w.r.t OCP.
- Application Deployment 
  1. Via image 
  2. via Image + source code (git)

- Kubernetes/OpenShift resources- 


#####################
1-) Login to OCP cluster using kubeadmin/password [kubeadmin account is "cluster-admin"]
2-) Users Creations.
    UserName   Password      FileName           SecretName    IdentityProviderName-(idp)    Group    
	user1-5    redhat123   sre-file-name        sre-secret       sre-team                  sre_grp
    user6-10   redhat678   devops-file-name    devops-secret     DevOps-team               devops_grp 
    user11-20  redhat11    app-file-name       app-secret        app-team                  app_grp 
	
3-) ALL work done via API only - using webConsole/oc.

Note: Identity provider is "htpasswd"
Other IDP are [ LDAP, Keystone, GitEnterprise, OpenID Connect-OIDC]

Step-1 - Create user/password file via htpasswd.
 
          $ sudo yum install httpd-tools -y 
          $ htpasswd -c -B -b sre-file-name user1 redhat123 
		  $ htpasswd -B -b sre-file-name user2 redhat123 
		  $ htpasswd -B -b sre-file-name user3 redhat123
		  $ htpasswd -B -b sre-file-name user4 redhat123
		  $ htpasswd -B -b sre-file-name user2 redhat123
		  $ cat sre-file-name
		  
		  
Step-2 - Create genric secret (via --from-file with tag "htpasswd"), w.r.t namespace="openshift-config".
         $ oc create secret generic sre-secret --from-file htpasswd=sre-file-name -n openshift-config 
         $ oc get secret sre-secret -n openshift-config
		 $ oc get secret sre-secret -n openshift-config -o json/yaml 
		 $ echo "Encrypted data" | base64 -d 
		 
Step-3 - Update to Oauth service.
        $ oc get oauth/cluster 
		or 
		$ oc get oauth cluster 
		
		$ oc get oauth/cluster -o yaml
		$ oc get oauth/cluster -o yaml > oauth.yaml 
		Now updating oauth.yaml file and update to OCP cluster.
		
		https://docs.redhat.com/en/documentation/openshift_container_platform/4.14
		https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html/authentication_and_authorization/index
		
		https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html/authentication_and_authorization/configuring-identity-providers#identity-provider-htpasswd-CR_configuring-htpasswd-identity-provider
		
		
	   $ vim oauth.yaml
       ****************
        Already some contents.
       ****************	   
	   - name: sre-team
         mappingMethod: claim
         type: HTPasswd
         htpasswd:
           fileData:
              name: sre-secret 
	
	   $ oc apply -f oauth.yaml 
	   $ oc get pods -n openshift-authentication  
	   $ watch oc get pods -n openshift-authentication
       
       $ oc get user (oc get po)
       $ oc get groups 	   